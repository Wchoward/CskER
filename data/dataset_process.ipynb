{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "import os, collections, json, random\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fm.py\n",
    "def load_file(filepath, ):\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        a = []\n",
    "        for i in f:\n",
    "            a.append(i.strip(' \\n'))\n",
    "        return a\n",
    "\n",
    "\n",
    "def load_dict_json(filepath, ):\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        a = json.load(f)\n",
    "    return a\n",
    "\n",
    "\n",
    "def save_file(filepath, lst):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for i in range(len(lst)):\n",
    "            f.write(lst[i])\n",
    "            if i != len(lst) - 1:\n",
    "                f.write('\\n')\n",
    "\n",
    "\n",
    "def save_dict_json(filepath, dic):\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(dic, ensure_ascii=False))\n",
    "\n",
    "\n",
    "def load_xml(filepath):\n",
    "    tree = ET.ElementTree()\n",
    "    tree = ET.parse(filepath)\n",
    "    children = tree.findall('DOC')\n",
    "    a = []\n",
    "    for i in children:\n",
    "        for j in i[0]:\n",
    "            a.append(j.text)\n",
    "    return a\n",
    "\n",
    "\n",
    "def get_filelist(dir, Filelist):\n",
    "    newDir = dir\n",
    "    if os.path.isfile(dir):\n",
    "        Filelist.append(dir)\n",
    "        # 只返回文件名\n",
    "        # Filelist.append(os.path.basename(dir))\n",
    "    elif os.path.isdir(dir):\n",
    "        for s in os.listdir(dir):\n",
    "            # 若需要忽略文件夹\n",
    "            # if s == \"xxx\":\n",
    "            # continue\n",
    "            newDir = os.path.join(dir, s)\n",
    "            get_filelist(newDir, Filelist)\n",
    "    return Filelist\n",
    "\n",
    "\n",
    "def get_dir_list(dir_path):\n",
    "    lst = []\n",
    "    for s in os.listdir(dir_path):\n",
    "        if os.path.isdir(os.path.join(dir_path, s)):\n",
    "            lst.append(s)\n",
    "    return lst\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        tokens = reader.readlines()\n",
    "    for index, token in enumerate(tokens):\n",
    "        token = token.rstrip('\\n')\n",
    "        vocab[token] = index\n",
    "    return vocab\n",
    "\n",
    "def save_vocabulary(vocab, vocab_path):\n",
    "        \"\"\"Save the tokenizer vocabulary to a directory or file.\"\"\"\n",
    "        index = 0\n",
    "        with open(vocab_path, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(vocab.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    # logger.warning(\"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
    "                    #                \" Please check that the vocabulary is not corrupted!\".format(vocab_file))\n",
    "                    index = token_index\n",
    "                writer.write(token + u'\\n')\n",
    "                index += 1\n",
    "        return (vocab_path,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SST-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/chwu/.cache/huggingface/modules/datasets_modules/datasets/sst/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff (last modified on Tue Nov 16 21:28:55 2021) since it couldn't be found locally at sst., or remotely on the Hugging Face Hub.\n",
      "Reusing dataset sst (/home/chwu/.cache/huggingface/datasets/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ad4e82a70b4a768716e2a8b4b6df4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sst_dataset = load_dataset(\"sst\", \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'tokens', 'tree'],\n",
       "        num_rows: 8544\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'tokens', 'tree'],\n",
       "        num_rows: 1101\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'tokens', 'tree'],\n",
       "        num_rows: 2210\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float label 转化\n",
    "def label_trans(x):\n",
    "    if x >= 0 and x <= 0.2:\n",
    "        return 0\n",
    "    if x > 0.2 and x <= 0.4:\n",
    "        return 1\n",
    "    if x > 0.4 and x <= 0.6:\n",
    "        return 2\n",
    "    if x > 0.6 and x <= 0.8:\n",
    "        return 3\n",
    "    if x > 0.8 and x <= 1:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取对应train, valid, test数据集\n",
    "def get_data(all_dataset, splitname):\n",
    "    sent = all_dataset[splitname]['sentence']\n",
    "    label = all_dataset[splitname]['label']\n",
    "    label = [label_trans(x) for x in label]\n",
    "    return [str(l) + '\\t' + s for l, s in zip(label, sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(sst_dataset, 'train')\n",
    "valid_data = get_data(sst_dataset, 'validation')\n",
    "test_data = get_data(sst_dataset, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'EmotionRecognition/SST-5'\n",
    "save_file(os.path.join(output_dir, \"data_train.txt\"), train_data)\n",
    "save_file(os.path.join(output_dir, \"data_val.txt\"), valid_data)\n",
    "save_file(os.path.join(output_dir, \"data_test.txt\"), test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMP-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('EmotionRecognition/SMP-2020/train.tsv', header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=5000, shuffle=True, stratify=data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('EmotionRecognition/SMP-2020/data_train.txt', sep='\\t', index=None, header=None)\n",
    "test.to_csv('EmotionRecognition/SMP-2020/data_test.txt', sep='\\t', index=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IECE-Emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(input_file):\n",
    "    df = pd.read_csv(input_file, header=None)\n",
    "    # df = shuffle(df)\n",
    "    X = df.iloc[:, [0]].values\n",
    "    y = df.iloc[:, [2]].values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(full_data, shuffle=False, ratio = 0.1):\n",
    "    n_total = len(full_data)\n",
    "    offset = int(n_total * ratio)\n",
    "    if n_total == 0 or offset < 1:\n",
    "        return [], [], full_data\n",
    "    if shuffle:\n",
    "        random.shuffle(full_data)\n",
    "    dev_set = full_data[:offset]\n",
    "    test_set = full_data[offset:offset*2]\n",
    "    train_set = full_data[offset*2:]\n",
    "    return train_set, dev_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_dataset('../dataset/Implicit_emo_dataset/all_data.csv')\n",
    "print(set(y.reshape(-1).tolist()))\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y = y.astype(str).astype(object)\n",
    "X = X.astype(object)\n",
    "y = y.reshape(-1,1)\n",
    "x = (y+'\\t'+X)\n",
    "x = x.reshape(-1)\n",
    "data = list(set(x.tolist()))\n",
    "fm.save_file('../dataset/Implicit_emo_dataset/data.txt', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, dev_set, test_set = split(data, shuffle=True)\n",
    "fm.save_file('../dataset/Implicit_emo_dataset/data_train.txt', train_set)\n",
    "fm.save_file('../dataset/Implicit_emo_dataset/data_val.txt', dev_set)\n",
    "fm.save_file('../dataset/Implicit_emo_dataset/data_test.txt', test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('EmotionRecognition/ISEAR/isear.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['SIT'].values\n",
    "y = data['Field1'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y.reshape(-1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y = y.astype(str).astype(object)\n",
    "X = X.astype(object)\n",
    "y = y.reshape(-1,1)\n",
    "# x = (y+'\\t'+X)\n",
    "# x = x.reshape(-1)\n",
    "# data = list(set(x.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(-1).tolist()\n",
    "y = y.reshape(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame({'label':y, 'text':X})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_data, test_size=0.2, random_state=7, stratify=df_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, test = train_test_split(test, test_size=0.5, random_state=7, stratify=test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('EmotionRecognition/ISEAR/data_train.txt', sep='\\t', index=None, header=None)\n",
    "val.to_csv('EmotionRecognition/ISEAR/data_val.txt', sep='\\t', index=None, header=None)\n",
    "test.to_csv('EmotionRecognition/ISEAR/data_test.txt', sep='\\t', index=None, header=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('EmotionRecognition/TEC/Jan9-2012-tweets-clean.txt', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[1].values\n",
    "y = data[2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({':: surprise': 3849,\n",
       "         ':: sadness': 3830,\n",
       "         ':: joy': 8240,\n",
       "         ':: disgust': 761,\n",
       "         ':: fear': 2816,\n",
       "         ':: anger': 1555})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{':: anger', ':: disgust', ':: fear', ':: joy', ':: sadness', ':: surprise'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y.reshape(-1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y = y.astype(str).astype(object)\n",
    "X = X.astype(object)\n",
    "X = X.reshape(-1).tolist()\n",
    "y = y.reshape(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame({'label':y, 'text':X})\n",
    "train, test = train_test_split(df_data, test_size=0.2, random_state=7, stratify=df_data['label'])\n",
    "val, test = train_test_split(test, test_size=0.5, random_state=7, stratify=test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('EmotionRecognition/TEC/data_train.txt', sep='\\t', index=None, header=None)\n",
    "val.to_csv('EmotionRecognition/TEC/data_val.txt', sep='\\t', index=None, header=None)\n",
    "test.to_csv('EmotionRecognition/TEC/data_test.txt', sep='\\t', index=None, header=None)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c569aad1df965643a94c14181614dd60c48eb9bc9d4487e7a0cc834bfdf98891"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('py3.6': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
